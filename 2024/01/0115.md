## 0115

https://www.thebyte.com.cn/architecture/history.html 读书笔记


- [0115](#0115)
- [第一章 云原生技术概论](#第一章-云原生技术概论)
  - [1.1 云计算的演进变革](#11-云计算的演进变革)
  - [1.3 云原生的定义](#13-云原生的定义)
  - [1.4 云原生的目标](#14-云原生的目标)
  - [1.5 云原生代表技术](#15-云原生代表技术)
    - [1.5.1 容器技术](#151-容器技术)
    - [1.5.2 微服务](#152-微服务)
    - [1.5.3 服务网格](#153-服务网格)
    - [1.5.4 不可变基础设施](#154-不可变基础设施)
    - [1.5.6 DevOps](#156-devops)
  - [1.6 以解决问题为目的推进架构演进](#16-以解决问题为目的推进架构演进)
- [第二章: 构建”足够快“的网络服务](#第二章-构建足够快的网络服务)
  - [2.2 HTTPS 请求阶段分析](#22-https-请求阶段分析)
  - [2.3 域名解析环节实践](#23-域名解析环节实践)
    - [2.3.4 使用 HTTPDNS](#234-使用-httpdns)
  - [2.4 HTTP 请求优化](#24-http-请求优化)
    - [2.4.2 使用 Protocol Buffers 序列化数据](#242-使用-protocol-buffers-序列化数据)
  - [2.5 HTTPS 原理及 SSL 层优化实践](#25-https-原理及-ssl-层优化实践)
    - [2.5.2 SSL 层优化实践](#252-ssl-层优化实践)
  - [2.6 网络拥塞控制概论与实践](#26-网络拥塞控制概论与实践)
- [第三章：深入 Linux 内核网络](#第三章深入-linux-内核网络)
  - [3.1 Linux 系统收包流程](#31-linux-系统收包流程)
  - [3.2 Linux 内核网络框架](#32-linux-内核网络框架)
    - [3.2.1 iptables 与 Netfilter](#321-iptables-与-netfilter)
    - [3.2.2 连接跟踪 conntrack](#322-连接跟踪-conntrack)
    - [3.2.3 快速数据路径 XDP](#323-快速数据路径-xdp)
- [第四章：负载均衡概论](#第四章负载均衡概论)
  - [4.3 四层负载均衡](#43-四层负载均衡)
    - [4.3.2 四层负载均衡高可用设计](#432-四层负载均衡高可用设计)
  - [4.4 七层负载均衡](#44-七层负载均衡)
    - [4.4.2 从负载均衡到网关](#442-从负载均衡到网关)
- [第五章 分布式事务](#第五章-分布式事务)
  - [5.1 从一致性开始](#51-从一致性开始)
  - [5.2 一致性与可用性的权衡](#52-一致性与可用性的权衡)
- [第六章：分布式共识](#第六章分布式共识)
  - [6.1 从共识开始](#61-从共识开始)
  - [6.2 拜占庭将军问题](#62-拜占庭将军问题)
  - [6.3 Paxos 算法](#63-paxos-算法)
  - [6.4 Raft 算法](#64-raft-算法)
    - [6.4.1 Leader 选举](#641-leader-选举)
- [第七章：容器技术概论](#第七章容器技术概论)
  - [7.2 以容器构建系统](#72-以容器构建系统)
  - [7.3 容器镜像](#73-容器镜像)
  - [7.5 容器间网络](#75-容器间网络)
  - [7.7 编排调度模型](#77-编排调度模型)


## 第一章 云原生技术概论

### 1.1 云计算的演进变革

- IaaS（Infrastructure as a service，基础设施即服务）的出现：通过按时计费的方式租借服务器（卖资源），将资本支出转变为运营支出，这使得云计算得以大规模兴起和普及。
- PaaS（Platform as a service，平台即服务）的出现：使开发者不必费心考虑操作系统和开发工具更新或者硬件维护，云服务提供商由 IaaS 阶段的卖资源进阶为卖服务。
- 开源 IaaS 的出现：云计算开始进入开源时代。
- 开源 PaaS 的出现：云计算开始推动容器技术兴起 （详见本书 7.1 容器编排之战)。
- FaaS（Function as a Service，功能即服务）的出现：通过 FaaS，物理硬件、虚拟机操作系统和 Web 服务器软件管理等等都由云服务提供商自动处理。无服务器（Serverless）的概念已经初现，开发者将无需再关注任何服务、资源等基础设施。   

话说 FaaS 都出现了，SaaS 没有吗。   

2013 年发生了一件影响深广的技术变革：Docker 降世。   

Docker 创新性地提出了镜像的概念，实现了一种新型的应用打包、分发和运行机制。   

基于容器技术的容器编排市场，则经历了 Mesos、Swarm、kubernetes 三家的一场史诗大战，最终以 kubernetes 全面胜利而告终。     


### 1.3 云原生的定义

云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。

这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。    

### 1.4 云原生的目标   

- 可用（Available）：通过各种机制来实现应用的高可用，以保证服务提供的连续性。
- 规模（Scale）：要求云原生服务能够适应不同的规模（包括但不限于用户规模/部署规模/请求量），并能够在部署时动态分配资源，以便在不同的规模之间快速和平滑的伸缩。典型场景如：
初创公司或新产品线快速成长，用户规模和应用部署规模在短时间内十倍百倍增长。
促销、季节性、节假日带来的访问量波动。
高峰时间段的突发流量等。
- 敏捷（Agility）：快速响应市场需求。
- 成本（Cost）：充分有效的利用资源。


### 1.5 云原生代表技术


#### 1.5.1 容器技术

2006 年，Google 推出 Process Container（进程容器），Process Container 的目的非常直白，它希望能够像虚拟化技术那样给进程提供操作系统级别的资源限制、优先级控制、资源审计能力和进程控制能力。带着这样的设计思路，Process Container 推出不久就进入了 Linux Kernel 主干，不过由于 container 这一命名在内核中具有许多不同的含义，为了避免代码命名的混乱，后来就将 Process Container 更名为了 Control Groups，简称 cgroups。     


2008 年 Linux Kernel 2.6.24 刚刚开始提供 cgroups 的同一时间，社区开发者将 cgroups 的资源管理能力和 Linux namespace（命名空间）的资源隔离能力组合在一起，形成了完整的容器技术 LXC（Linux Container，Linux 容器），这就是如今被广泛应用的容器技术的实现基础。    

容器镜像将应用运行环境，包括代码、依赖库、工具、资源文件和元信息等，打包成一种操作系统发行版无关的不可变更软件包，从而实现一种新型的应用打包、分发和运行机制。    

![docker](https://www.thebyte.com.cn/assets/docker-arc-tAtb6WCf.png)

如果说以 Docker 为代表的容器引擎，是把软件的发布流程从分发二进制安装包，转变为了直接分发虚拟化后的整个运行环境，让应用得以实现跨机器的绿色部署。那以 Kubernetes 为代表的容器编排框架，就是把大型软件系统运行所依赖的集群环境也进行了虚拟化，让集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩缩。

#### 1.5.2 微服务

微服务架构首先是一个分布式的架构，而分布式意味着复杂性的挑战，软件架构从巨石应用向微服务架构转型的过程中带来了一系列的非功能性需求，譬如：   

1. 服务发现（Service Discovery）问题，解决“我想调用你，如何找到你”的问题。
2. 服务熔断（Circuit Breaker）问题，缓解服务之间依赖的不可靠问题。
3. 负载均衡（Load Balancing）问题，通过均匀分配流量，让请求处理更加及时。
4. 安全通讯问题，包括协议加密（TLS）、身份认证（证书/签名）、访问鉴权（RBAC）等

#### 1.5.3 服务网格    

Service Mesh 是一个处理服务通讯的专门的基础设施层。它的职责是在由云原生应用组成服务的复杂拓扑结构下进行可靠的请求传送。在实践中，它是一组和应用服务部署在一起的轻量级的网络代理，对应用服务透明。     

所以这个东西一般和应用部署在一起，专门处理应用间的网络通信？     

**Sidecar**    

ServiceMesh 之所以称为服务网格，是因为每台节点同时运行着业务逻辑和代理，这个代理被形象地称为 Sidecar（业务逻辑相当于主驾驶，共享一个代理相当于边车）。服务之间通过 Sidecar 发现和调用目标服务，从而在服务之间形成一种网络状依赖关系，如果我们把节点和业务逻辑从视图剥离，就会出现一种网络状的架构，如图 1-23 所示，服务网格由此得名。

![service mesh](https://www.thebyte.com.cn/assets/service-mesh-7BkRiw65.png)

服务网格的实现通常由两部分组成：数据平面和控制平面，以服务网格的代表实现 Istio 架构为例，如图 1-24 所示：

![istio](https://www.thebyte.com.cn/assets/service-mesh-arc-aKNYiMfs.svg)

- 数据平面（Data plane）通常采用轻量级的代理（例如 Envoy）作为 Sidecar，这些代理负责协调和控制服务之间的通信和流量处理。
- 控制平面（Control plane）负责配置和管理数据平面，并提供服务发现、智能路由、流量控制、安全控制等功能。

那这控制平面不就把微服务的非功能性需求都解决了吗。    

Sidecar 本质上是一个服务代理，通过劫持发送到应用容器的流量从而实现对流量的控制，随着服务网格落地实践，Sidecar 的缺点也逐渐被暴露：   

- 延迟问题：尽管从一些产品的 benchmark 结果来看，Sidecar 的引入只会增加毫秒级（个位数）延迟，但对性能有极高要求的业务场景，来说，延迟损耗成为了放弃服务网格的最主要原因。
- 资源占用问题：Sidecar 作为一个独立的容器必然会占用一定的系统资源，对于超大规模集群（如数万个 Pod）来说，巨大的基数也使得资源总量变成了不小的数目，同时，这类集群的网络通信拓扑也更加复杂，配置下发的规模也会让 Sidecar 的内存出现剧烈的增长。    

#### 1.5.4 不可变基础设施

不可变基础设施的核心思想是任何基础设施的实例一旦创建之后就变成只读状态。如需修改或升级，应该先修改基础设施的配置模版（例如 yaml 配置），修改配置模版之后使用新的实例进行替换。    

不可变基础设施的系统中如果有新的变更需求，如上面的 Nginx 升级案例，就应该准备一个新的 Nginx 基础设施，而不是在原有的基础上做原地更新。    

对比可变基础设施，不可变基础设施的最大的优势是一致性，在不可变基础设施下，所有的配置都通过标准化描述文件（例如 yaml、dockerfile 等）进行统一定义，不同的 Pod、Service 都按照同样的定义创建，不同实例配置不一致的情况不会再出现。在一致性的前提下，当线上突发故障或者遇到异常流量时，不可变基础设施才可以快速进行弹性扩缩容、升级、回滚等操作，应对问题时也更加快速和自动化。     

#### 1.5.6 DevOps

虽然敏捷开发提升了开发效率，但它的范围仅限于开发和测试阶段，并没有覆盖到部署端。很显然，运维部门并没有在这其中得到收益，相反地，甚至可以说“敏捷”加重了运维的负担。因为运维追求的目标是稳定，而频繁的变更往往就是出现问题的根源。     

### 1.6 以解决问题为目的推进架构演进

- 为了解决单体架构 “复杂度问题”，使用微服务架构。
- 为了解决微服务间 “通讯异常问题”，使用治理框架 + 监控。
- 为了解决微服务架构下大量应用 “部署问题”，使用容器。
- 为了解决容器的 “编排和调度问题”，使用 Kubernetes。
- 为了解决微服务框架的 “侵入性问题”，使用 Service Mesh。
- 为了让 Service Mesh 有 “更好的底层支撑”，将 Service Mesh 运行在 Kubernetes 上。    


从单个微服务应用的角度看，自身的复杂度降低了，在“强大底层系统”支撑的情况下监控、治理、部署、调度功能齐全，已经符合云原生架构。但站在整个系统的角度看，复杂度并没有减少和消失，要实现“强大底层系统”付出的成本是非常昂贵（很强的架构和运维能力）。     

为了解决项目整体复杂度，选择上云托管，将底层系统的复杂度交给云基础设施，让云提供保姆式服务，最终演变为无基础架构设计，通过 YAML 声明式代码、编排底层基础设施、中间件等资源，即应用要什么，云给我什么，企业最终会走向开放、标准的“云”技术体系。

## 第二章: 构建”足够快“的网络服务

### 2.2 HTTPS 请求阶段分析

一个完整、无任何缓存、未复用连接的 HTTPS 请求需要经过以下几个阶段：DNS 域名解析、TCP 握手、SSL 握手、服务器处理、内容传输。     

![http request](https://www.thebyte.com.cn/assets/http-process-zCDWkVFw.png)     

一个 HTTPS 请求共需要 5 个 RTT = 1 RTT（域名解析）+ 1 RTT（TCP 握手）+ 2 RTT（SSL 握手）+ 1 RTT（HTTP 内容请求传输）。    

### 2.3 域名解析环节实践

nslookup 命令，该命令用于查询 DNS 的记录、域名解析是否正常等。    

```
$ nslookup thebyte.com.cn        
Server:		8.8.8.8
Address:	8.8.8.8#53

Non-authoritative answer:
Name:	thebyte.com.cn
Address: 110.40.229.45
```     

- 第一行的 Server 为当前使用的 DNS解析器。
- Non-authoritative answer 因为 DNS 解析器只是转发权威解析服务器的记录，所以为非权威应答。
- Address 为解析结果，上面的解析可以看到是一个A记录 110.40.229.45。    

本机的例子：    

```
$ nslookup www.baidu.com
Server:		30.30.30.30
Address:	30.30.30.30#53

Non-authoritative answer:
www.baidu.com	canonical name = www.a.shifen.com.
Name:	www.a.shifen.com
Address: 220.181.38.150
Name:	www.a.shifen.com
Address: 220.181.38.149
```   


#### 2.3.4 使用 HTTPDNS

简而言之，HTTPDNS 就是使用 HTTP 协议直接向 DNS 的权威解析服务器进行请求，从而获取域名对应的 IP 地址。HTTPDNS 跳过默认系统 DNS 解析的过程，使用 HTTP(S) 协议绕过运营商的 Local DNS，从而避免域名劫持，也更准确地判断客户端地区和运营商，得到更精准的解析结果。    

但是这东西不是前端应用能使用的技术吧。可能移动端app可以这样搞？也不是很确定，至少web浏览器是没有暴露这样的接口。   


### 2.4 HTTP 请求优化

- 包体积优化：传输数据的包体大小与传输耗时成正相关，压缩算法是减小包体的最有效手段(没有之一)。
- SSL 层优化：升级 TLS 算法以及 HTTPS 证书，降低 SSL 层的性能消耗。
- 传输层优化：升级拥塞控制算法（例如由默认的 Cubic 升级为 BBR 算法）提升数据传输效率。
- 网络层优化：使用一些商业网络加速服务，在网络层对数据包进行路由优化，实现动态服务加速。
- 使用更现代的 HTTP 协议：升级至 HTTP/2，进一步可以使用 QUIC。

#### 2.4.2 使用 Protocol Buffers 序列化数据

Protocol Buffers（简称 Protobuf 或者 pb）是 Google 公司开发的一种轻便高效的结构化数据存储格式，用来描述各种数据结构进行结构化数据串行化，或者说序列化。相比 XML 和 JSON，Protobuf 更小、更快、更简单，很适合做数据存储或 RPC 数据交换格式。

在 Protobuf 中，所有结构化的数据都被称为 message。    

如果开头第一行不声明 syntax = "proto3"; 则默认使用 proto2 进行解析   

```
syntax = "proto3"; // pb 版本声明

message Person  { 
  string name = 1;
  int32 age = 2;
}
```     

定义完 message 结构体之后，生成 java 可调用的类文件。    

```
proto --java_out = / Person.proto
```     

在 Java 中使用 Protobuf 定义的结构进行序列化数据（需要提前安装 protobuf 依赖库）。     

```
//1、 创建Builder
PersonProto.Person.Builder builder = PersonProto.Person.newBuilder();
//2、 设置Person的属性
builder.setAge(35);
builder.setName("小李");
//3、 创建Person
PersonProto.Person person = builder.build();
//4、序列化
byte[] data = person.toByteArray();
```     

序列化后的数据，可以通过 RPC、HTTP 等方式传递，接收方获取数据后进行反序列化。    

```
PersonProto.Person person = PersonProto.Person.parseFrom(data);
System.out.println(person.getAge());
System.out.println(person.getName());
```     

老实说没太看懂。msg 到底本身就是数据，还是一个数据的结构定义。   


### 2.5 HTTPS 原理及 SSL 层优化实践

#### 2.5.2 SSL 层优化实践    

TLS 1.3 协议中的 TLS 握手只需要一次 RTT 而不是两次，如果客户端复用之前连接，TLS 握手的往返次数可以为零。    


### 2.6 网络拥塞控制概论与实践

BBR (Bottleneck Bandwidth and Round-trip propagation time)是 Google 在 2016 年发布的一套拥塞控制算法。它尤其适合在存在一定丢包率的弱网环境下使用，在这类环境下，BBR 的性能远超 Cubic 等传统的拥塞控制算法。    

1980 年代的拥塞控制算法分为四个部分：慢启动、拥塞避免、快速重传、快速恢复。     

![flowcontrol](https://www.thebyte.com.cn/assets/cc-15XEkKgx.png)      

BBR 的设计思路是不再考虑丢包作为拥塞的判断条件，而是交替测量带宽和延迟，用一段时间内的带宽极大值和延迟极小值作为估计值。BBR 将控制时机提前，不再等到丢包时再进行暴力限制，而是控制稳定的发包速度，尽量榨干带宽，却又不让报文在中间设备的缓存队列上累积。为了榨干带宽，BBR 会周期性地去探测是否链路条件变好了，如果是，则加大发送速率。为了不让报文在中间设备的缓存队列上累积，BBR 会周期性地探测链路的最小 RTT，并使用该最小 RTT 计算发包速率。

## 第三章：深入 Linux 内核网络

### 3.1 Linux 系统收包流程

![eth](https://www.thebyte.com.cn/assets/networking-2FUmpNiv.svg)    

1. 网卡 eth0 收到数据包。
2. 网卡通过 DMA 将数据包拷贝到内存的环形缓冲区(Ring Buffer，在网卡中有 RX Ring 和 TX Ring 两种缓冲)。
3. 数据从网卡拷贝到内存后, 网卡产生 IRQ（Interupt ReQuest，硬件中断）告知内核有新的数据包达到。
4. 内核收到中断后, 调用相应中断处理函数，开始唤醒 ksoftirqd 内核线程处理软中断。
5. 内核进行软中断处理，调用 NAPI poll 接口来获取内存环形缓冲区(ring buffer)的数据包，送至更上层处理。
6. 内核中网络协议栈：L2 处理。
7. 内核中网络协议栈：L3 处理。
8. 内核中网络协议栈：L4 处理。
9. 网络协议栈处理数据后，并将其发送到对应应用的 socket 接收缓冲区。       

### 3.2 Linux 内核网络框架    

从 Linux Kernel 2.4 版本起，内核就开放了一套通用的，可提供代码干预数据在协议栈流转的过滤框架 -- Netfilter。     

Netfilter 实际上就是一个过滤器框架，Netfilter 在网络包收发以及路由的“管道”中，一共切了 5 个口（hook），分别是 PREROUTING、FORWARD、POSTROUTING、INPUT 以及 OUTPUT，其它内核模块(例如 iptables、IPVS 等)可以向这些 hook 点注册处理函数。每当有数据包留到网络层，就会自动触发内核模块注册在这里的回调函数，这样程序代码就能够通过回调函数来干预 Linux 的网络通信，进而实现对数据包过滤、修改、SNAT/DNAT 等各类功能。    

Kubernetes 集群服务的本质其实就是负载均衡或反向代理，而实现反向代理，归根结底就是做 DNAT，即把发送给集群服务的 IP 地址和端口的数据包，修改成具体容器组的 IP 地址和端口。      

#### 3.2.1 iptables 与 Netfilter

Netfilter 的钩子回调固然强大，但仍要通过程序编码才能使用，并不适合系统管理员日常运维，而设计 iptables 的目的便是以配置的方式实现原本用 Netfilter 编码才能做到的事情。iptables 在用户空间管理数据包处理规则，内核中 Netfilter 根据 iptables 的配置对数据包进行处理。     

iptables 包括了“tables（表）”、“chain（链）”和“rules（规则）”三个层面。iptables 使用表来组织规则，如果规则是处理网络地址转换的，那会放到 nat 表，如果是判断是否允许包继续向前，那可能会放到 filter 表。每个表内部规则被进一步组织成链，链由内置的 hook 触发后数据包依次匹配里面的规则。    

这几个内置链的功能如下：   

- PREROUTING: 接收到的包进入协议栈后立即触发此链，在进行任何路由判断（将包发往哪里）之前。
- INPUT: 接收到的包经过路由判断，如果目的是本机，将触发此链。
- FORWARD 接收到的包经过路由判断，如果目的是其他机器，将触发此链。
- OUTPUT: 本机产生的准备发送的包，在进入协议栈后立即触发此链。
- POSTROUTING: 本机产生的准备发送的包或者转发的包，在经过路由判断之后，将触发此链。     

一个目的是本机的数据包依次经过 PREROUTING 链上面的 mangle、nat 表，然后再依次经过 INPUT 链的 mangle、filter、nat表，最后到达本机某个具体应用。

![table,chain](https://www.thebyte.com.cn/assets/iptables-chain-7LETADq7.png)     

#### 3.2.2 连接跟踪 conntrack

连接跟踪（connection tracking，conntrack，CT）对连接状态进行跟踪并记录。如图 3-11 所示，这是一台 IP 地址为 10.1.1.3 的 Linux 机器，我们能看到这台机器上有两条连接：   

机器访问外部 HTTP 服务的连接（目的端口 80）。
机器访问外部 DNS 服务的连接（目的端口 53）。

![ct](https://www.thebyte.com.cn/assets/conntrack-iYHhimB6.png)     

连接跟踪所做的事情就是发现并跟踪这些连接的状态，具体包括：    

- 从数据包中提取元组信息，辨别数据流和对应的连接。
- 为所有连接维护一个状态数据库（conntrack table），例如连接的创建时间、发送 包数、发送字节数等。
- 回收过期的连接（GC）。
- 为更上层的功能（例如 NAT）提供服务。

conntrack（椭圆形方框）在内核中有两处位置（PREROUTING 和 OUTPUT之前）能够跟踪数据包。    

每个通过 conntrack 的数据包，内核都为其生成一个 conntrack 条目用以跟踪此连接，对于后续通过的数据包，内核会判断若此数据包属于一个已有的连接，则更新所对应的 conntrack 条目的状态(比如更新为 ESTABLISHED 状态)，否则内核会为它新建一个 conntrack 条目。所有的 conntrack 条目都存放在一张表里，称为连接跟踪表（conntrack table）。     

conntrack 的应用。机器自己的 IP 10.1.1.3 可以与外部正常通信，但 192.168 网段是私有 IP 段，外界无法访问，源 IP 地址是 192.168 的包，其应答包也无法回来，因此：

- 当源地址为 192.168 网段的包要出去时，机器会先将源 IP 换成机器自己的 10.1.1.3 再发送出去，进行 SNAT（对源地址 source 进行 NAT）。
- 收到应答包时，再进行相反的转换，进行 DNAT（对目的地址 destination 进行 NAT）。    

![nat](https://www.thebyte.com.cn/assets/nat-TpmnSAHs.png)    


#### 3.2.3 快速数据路径 XDP

由于 Linux 内核协议栈更加注重通用性，所以在网络性能需求场景中存在一定的瓶颈，随着 100G、200/400G 高速率网卡的出现，这种性能瓶颈就变得无法接受了。2010 年，由 Intel 领导的 DPDK 实现了一个基于内核旁路（Bypass Kernel）思想的高性能网络应用开发解决方案，并逐渐成为了独树一帜的成熟技术体系。但是 DPDK 也由于内核旁路这一前提，天然就无法与内核技术生态很好的结合。     

2016 年，伴随着 eBPF 技术的成熟，Linux 内核终于迎来了属于自己的高速公路 —— XDP（AF_XDP），其具有足以媲美 DPDK 的性能以及背靠 Linux 内核的多种独特优势。    

## 第四章：负载均衡概论     

负载均衡器完成以下功能：  

- 服务发现：系统中哪些后端可用、它们的地址是什么（负载均衡器如何能够联系上它们）。
- 健康检查：哪些后端是健康的，可以正常接收请求。
- 负载均衡：用哪种算法来均衡请求至健康的后端。    

![lb](https://www.thebyte.com.cn/assets/balancer-uRgYIjFv.svg)    


网络类型所有解决方案通常分为两类：四层负载均衡和七层负载均衡，这两者分别对应 OSI 模型的 4 层和 7 层。       

### 4.3 四层负载均衡

四层负载均衡器最典型的软件实现是 LVS（Linux Virtual Server，Linux 虚拟服务器）。    

**链路层负载均衡**     

LVS 的 DR 模式其实是一种链路层负载均衡，数据链路层负载均衡所做的工作，就是修改请求的数据帧中的 MAC 目标地址，让原本发送给负载均衡器请求的数据帧，被二层交换机转发至服务器集群中对应的服务器，这样真实的服务器就获得了一个原本目标并不是发送给它的数据帧。由于链路层只修改了 MAC 地址，所以在 IP 层看来数据没有任何变化，继而被正常接收。由于 IP 数据包中包含了源（客户端）和目的地（负载均衡器）IP 地址，只有真实服务器保证自己的 IP 与 数据包中的目的地 IP 地址一致，这个数据包才能继续被处理。      

因此使用这种负载均衡模式时，需要把真实服务器的 VIP 配置成和负载均衡器的 VIP 一样，LVS 配置真实服务器的原理也是如此，如下示例。     

只有请求经过负载均衡器，而服务的响应无需从负载均衡器原路返回的工作模式中，整个请求、转发、响应的链路形成一个“三角关系”。     

虽然 DSR 模式效率很高，但它在缺陷也很明显，因为响应不再经过负载均衡器，负载均衡器就无法知道 TCP 连接的完整状态，防火墙策略会受到影响（TCP 连接只有 SYN，没有 ACK）。其次因为是数据链路层通信，受到的网络侧约束也很大。所以优势（效率高）和劣势（不能跨子网）共同决定了 DSR 模式最适合作为数据中心的第一层均衡设备，用来连接其他的下级负载均衡器。

**网络层负载均衡**    

LVS 的 Tunnel、NAT 模式都属于网络层负载均衡，只不过因为对 IP 数据包的不同形式修改而分成两种，我们先来看第一种修改方式。

第一种保持源数据包不变，新建一个 IP 数据包，将原来的 IP 数据包整体放进新 IP 数据包的 Payload 内，再通过三层交换机发送出去，真实服务器收到包之后，有一个对应拆包的机制，把负载均衡器自动添加的那层 Header 删掉，解析出 Payload 内部的 IP 数据包再进行正常处理。把一个数据包封装在另一个数据包内，其实就是一种隧道技术，比如 Linux 中的隧道技术 ipip 就是字面 IP in IP 的意思。由于 IP 隧道工作在网络层，因此摆脱了直接路由模式的网络约束，所以 LVS Tunnel 模式可以跨越 VLAN 。   

由于没有修改源数据包的任何信息，所以 IP 隧道模式仍具有三角传输模式的特点，即负载均衡转发器转发进来的请求，真实服务器去响应请求，IP 隧道模式从请求到响应的过程如图所示。    

![tunnel](https://www.thebyte.com.cn/assets/balancer4-tunnel-1a728gV0.svg)    


IP 隧道模式相当于 DR 模式的升级（支持了跨网），不过由于使用隧道模式，所以要求真实服务器支持隧道协议，真实服务器只局限在部分 Linux 系统上。其次，只要是三角模式（LVS 的 DR 模式或者 Tunnel 模式）必须要保证真实服务器与负载均衡服务器有相同的虚拟 IP 地址，因为回复客户端时，必须使用这个虚拟的 IP 作为数据包的源地址，这样客户端收到数据包之后才能正常解析；最后，因为真实服务器和客户端对接，所以真实服务器得能访问外网。

IP 数据包的另外一种改写方式是直接改变 IP 数据包的 Header 内的目的地（Destination address）地址（改为真实服务器的地址），修改之后原本用户发送给负载均衡器的数据包会被三层交换机转发至真实服务器的网卡上。这么解释似乎不太好理解，但相信每一个读者都配置过这种原理的操作（配置路由器），这种模式和一台路由器带着一群内网机器上网的“网络地址转换”（Network Address Translation）一个原理。因此，这种负载均衡的模式被称为 NAT 模式。    

![nat](https://www.thebyte.com.cn/assets/balancer4-NAT-RyTeoyh2.svg)        

这种类型中，TCP 连接在负载均衡器建立连接跟踪和网络地址转换（NAT）之后直接转发给选中的后端。例如，假设客户端正在和负载均衡器 1.1.1.1:80 通信，选中的后端是 10.0.0.2:8080。当客户端的 TCP 包到达负载均衡器时，负载均衡器会将包的目的 IP/port （从 1.1.1.1:80）换成 10.0.0.2:8080，以及将源 IP/port 换成负载均衡器自己的 IP/port。当应答包回来的时候，负载均衡器再做相反的转换。    

不过 NAT 模式的缺陷也显而易见，因为负载均衡器代表整个服务集群接收/应答，当流量压力比较大的时候，整个系统的瓶颈就很容易出现在负载均衡器上。    

#### 4.3.2 四层负载均衡高可用设计

基于集群的一致性哈希容错和可扩展设计方案。它的工作原理如图所示。    

![ha](https://www.thebyte.com.cn/assets/balancer-ha-2-IBw9_ApD.svg)    

看不懂。     

### 4.4 七层负载均衡

一个七层 HTTP/2 负载均衡器，此时客户端、负载均衡器、真实服务器的工作模式。    

![lb](https://www.thebyte.com.cn/assets/balancer7-emTiJ0WH.svg)

这种情况下，客户端与七层负载均衡器建立一个 HTTP /2 TCP 连接，后续负载均衡器根据负载均衡策略和两个后端建立了连接。当客户端向负载均衡器发送两个 HTTP/2 流（streams ）时，stream 1 会被发送到 backend-1，而 stream 2 会被发送到 backend-2。因此，即使不同客户端的请求数量差异巨大，这些请求也可以被高效平衡的分发到后端。七层负载均衡具备检测应用层流量的能力，就能做出更多、更明智的决策，也能玩出更多的花样    

七层负载均衡的实现相信读者们已经非常熟悉，没错，它就是 Nginx。使用 Nginx 七层负载均衡能识别应用层协议，可以通过对 HTTP 头、URL、Cookie 做复杂的逻辑处理，实现更灵活的控制。互联网技术架构中，通常 7 层负载均衡的核心是 Nginx，结合 Lua 插件技术，如 OpenResty，能扩展实现功能丰富且性能较高的网关方案。    

#### 4.4.2 从负载均衡到网关

随着微服务架构、容器编排调度等技术的崛起，现代分布式系统已经越来越动态，这就意味着通过静态文件配置方式早已过时。作为分布式系统的入口，负载均衡器就需要转换角色，不仅仅作为一个代理，而是要承担提供更多现代化的功能。把这些功能统一前移某一层独立支持，实现 API Gateway as a Service，让各个服务直接接入，在管理平台上管理，可视化配置等等，这样就实现了一个全局的视图统一管理这些功能。     

而这就是七层负载均衡的升级 -- 网关（API Gateway）。     

## 第五章 分布式事务

### 5.1 从一致性开始

事务存在的意义就是保证系统中不同数据间不会产生矛盾，数据修改后的结果是我们期望的，也就是保证数据状态的一致性（Consistency）。想要达成数据状态的一致性，需要三个方面的努力：

- 原子性（Atomic）：原子化更新。业务操作要么全部完成, 要么全都不做，不容忍中间状态。
- 隔离性（Isolation）：在不同的业务处理过程中，事务保证了各自业务正在读、写的数据互相独立，不会彼此影响。
- 持久性（Durability）：业务数据不会丢失。事务结束后，除非数据被其他业务更新，否则不应该有变化。     

这也就是常说的事务的 ACID 特性，不过笔者这里额外提一点，事实上对于一致性而言，A、I、D 是手段，C（Consistency）是三者协作的目标，弄到一块完全是为了读起来顺口。     

一致性强弱的程度关乎系统设计权衡，由此事务处理从一个具体操作上的“编程问题”而转变成一个需要全局权衡的“架构问题”。人们在探索这些架构的设计时产生了诸多思路和理论，下面笔者就来介绍分布式系统中最为出名的权衡理论 -- CAP 定理。

### 5.2 一致性与可用性的权衡

CAP 定理描述的是一个分布式系统中，涉及共享数据问题时，以下三个特性最多只能同时满足其中两个。    

- 一致性（Consistency）：代表数据在任何时刻、任何分布式节点中所看到的都是符合预期的。基于证明严谨性的考虑，在学术的研究中一致性指的是强一致性（Strong Consistency）或者也叫做线性一致性（Linearizability）。
- 可用性（Availability）：代表系统不间断地提供服务的能力，理解可用性要先理解与其密切相关两个指标：可靠性（Reliability）和可维护性（Serviceability）。可靠性使用平均无故障时间（Mean Time Between Failure，MTBF）来度量；可维护性使用平均可修复时间（Mean Time To Repair，MTTR）来度量。可用性衡量系统可以正常使用的时间与总时间之比，其表征为：A=MTBF/（MTBF+MTTR），即可用性是由可靠性和可维护性计算得出的比例值，譬如 99.9999%可用，即代表平均年故障修复时间为 32 秒。
- 分区容错性（Partition tolerance）：代表分布式环境中部分节点因网络原因而彼此失联后，即与其他节点形成“网络分区”时，系统仍能正确地提供服务的能力。    

对一个分布式系统，因为必须要实现分区容错性，不能舍弃 P，CAP 其实是二选一（AP 和 CP 之间）而不是三选二而的权衡。     

分析如果舍弃 C、A、P 时所带来的不同影响。     

- 放弃分区容忍性（CA without P）：意味着我们将假设节点之间通信永远是可靠（注意，笔者开篇的分布式八大缪误），永远可靠的通信在分布式系统中必定不成立，只要用到网络来共享数据，分区现象就会始终存在。没有 P，也谈不上是什么分布式系统。
- 放弃可用性（CP without A）：意味着我们将假设一旦网络发生分区，节点之间的信息同步时间可以无限制地延长。在现实中，选择放弃可用性的 CP 系统情况一般用于对数据质量要求很高的场合中。也就是说，可能由于出现网络分区了，系统为了保证一致性，涉及这部分的服务可能就不可用了，但一定是保证数据是准的。
- 放弃一致性（AP without C）：意味着我们将假设一旦发生分区，节点之间所提供的数据可能不一致。选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择，因为 P 是分布式网络的天然属性，你再不想要也无法丢弃；而 A 通常是建设分布式的目的，如果可用性随着节点数量增加反而降低的话，分布式系统也就失去了存在的价值，除非银行、证券这些涉及金钱交易的服务，宁可中断也不能出错，否则多数系统是不能容忍节点越多可用性反而越低。

## 第六章：分布式共识

### 6.1 从共识开始

在分布式系统中，为了消除单点提高系统可用性，有一种使用副本形式的容错模型。这种容错模型使用多个成员组成集群，成员之间数据完全一致（所以也叫副本），使用副本形式的分布式容错系统可以保证即使在小部分（≤ (N-1)/2）节点故障的情况下，系统仍然能正常对外提供服务。复制状态机是分布式系统中实现以上容错机制的基本方法，而共识算法（Consensus Algorithm）是构建复制状态机系统最关键的角色。     

复制状态机（Replicated State Machine）是指多台机器具有完全相同的状态，运行完全相同的确定性状态机。它让多台机器协同工作犹如一个强化的组合，其中少数机器宕机不影响整体的可用性。     

一个复制状态机副本的工作流程：

1. 客户端请求。
2. 共识模块执行共识算法进行日志复制，将日志复制至集群内各个节点。
3. 日志应用到状态机。
4. 服务端返回请求结果。    

### 6.2 拜占庭将军问题     

拜占庭帝国派出多支军队去围攻一个强大的敌人，每支军队有一个将军，但由于彼此距离较远，他们之间只能通过信使传递消息。敌方很强大，必须有超过半数的拜占庭军队一同参与进攻才可能击败敌人。在此期间，将军们彼此之间需要通过信使传递消息并协商一致后，在同一时间点发动进攻。

拜占庭将军问题恰恰在此：在一致性的达成过程中，叛徒将军（恶意节点）甚至不需要超过半数，就可以破坏占据多数正常节点一致性的达成，这也是我们常说的二忠一叛难题。        


话说为什么自己的系统要考虑叛徒的问题啊，这是什么奇怪的玩意啊。    

**口信消息型解决方案**     

对于口信消息(Oral message)的定义如下：    

- 任何已经发送的消息都将被正确传达。
- 消息的接收者知道是谁发送了消息。
- 消息的缺席可以被检测。     

在口信消息型解决方案中，首先发送消息的将军称为指挥官，其余将军称为副官。对于 3 忠 1 叛的场景需要进行两轮作战信息协商，如果没有收到作战信息那么默认撤退。    

- 第一轮 指挥官向 3 位副官发送了进攻的消息。
- 第二轮 三位副官再次进行作战信息协商，由于将军 A、B 为忠将，因此他们根据指挥官的消息向另外两位副官发送了进攻的消息，而将军 C 为叛将，为了扰乱作战计划，他向另外两位副官发送了撤退的消息。最终指挥官、将军 A 和 B 达成了一致的进攻计划，可以取得胜利。    

有点疑问，这时候 A, B 发出来的消息，属于转发还是自己的意见。根据下面的例子来看，是属于转发的。    

如果叛将人数为 m，将军人数不少于 3m+1，那么最终能达成一致的行动计划。值的注意的是，在这个算法中，叛将人数 m 是已知的，且叛将人数 m 决定了递归的次数，即叛将数 m 决定了进行作战信息协商的轮数，如果存在 m 个叛将，则需要进行 m+1 轮作战信息协商。这也是上述存在 1 个叛将时需要进行两轮作战信息协商的原因。

**签名消息型解决方案**    

对签名消息的定义是在口信消息定义的基础上增加了如下两条：

- 忠诚将军的签名无法伪造，而且对他签名消息的内容进行任何更改都会被发现。
- 任何人都能验证将军签名的真伪。    

本质上这种，你需要转发你收到的消息。    

签名消息型解决方案下，无论叛徒如何捣乱，将军 A、B都能执行一致的作战计划，需要注意，签名消息的拜占庭将军问题之解，也是需要 m+1（m为叛将数量）轮协商。我们也可以从另外一个角度理解：n 位将军，能容忍（n-2）为叛将。     

### 6.3 Paxos 算法

Paxos 算法包含两个部分：其中一部分是核心算法（Basic Paxos）；另外一部分是基于核心算法扩展的完整算法（Multi Paxos）。在笔者看来 Basic Paxos 是 Multi-Paxos 思想的核心，说直接点 Multi-Paxos 就是多执行几次 Basic Paxos，所以掌握了 Basic Paxos，我们便能更好的理解后面基于 Multi-Paxos 思想的共识算法（譬如 raft 算法）。     

从前有个村，老村长退休了，需要选一个新村长，选取新村长的事件称之为提案（Proposal）。张三、李四都想当村长（张三、李四我们称为提议节点，Proposer），但当村长需要多位村委（决策节点，Acceptor）的投票同意，村委使用少数服从多数（Quorum）投票机制。选举结束之后，要把结果同步给村民（记录节点，Learner）。    

实现多个节点 x 值一致的复杂度主要来源于以下两个因素的共同影响：

- 系统内部各个节点的通信是不可靠的。不论是系统中企图设置数据的提议节点，还是决定操作是否批准的决策节点，其发出、收到的信息都可能存在延迟、丢失的情况。
- 客户端写入是并发的，如果是串行的修改数据，仅单纯使用 Quorum 机制，少数节点服从多数节点，就足以保证数据被正确读写。而并发访问就变成了 “分布式环境下多个节点并发操作共享数据” 的问题。    


我们把上面的背景问题总结转化，其实就是下面两个核心需求：

- 安全性 Safety：
  + 一个变量只会被确定一个值（只能一个人当村长），一个变量只有在值被确定之后，才能被学习。
- 活性 Liveness：
  + 提案最终会被接受（一定能选出个村长）；一个提案被接受之后，最终会被所有的村民（learner）学习到。
  + 必须在有限时间内做出决议（不能有太多轮投票）。    

感觉这里的 Proposer 可以理解为数据 input，客户发来的数据请求，Acceptor 相当于控制节点。Learner 相当于集群的其他服从节点？     

也就是说，我们讨论的 Basic-Paxos 只会 Chosen 一个值。基于此，就需要一个两阶段（2-phase）协议，对于已经 Chosen 的值，后面的提案要放弃自己的提议，提出已经被选中的值。例如，S5 发起提案之前，先广播给 S3、S4、S5 这 3 个节点，询问是否已经有接受的提案，如果已有，则撤销自己的提案，S5 的题案由 blue 改为 red，这一阶段在 Basic Paxos 称为准备（Prepare）阶段。     

看不懂看不懂。    


### 6.4 Raft 算法

#### 6.4.1 Leader 选举

Raft 算法中，节点有三种状态：

- leader：领导者，接收 client（客户端）的所有请求，raft 算法中所有的操作以 leader 为准。领导者平常的工作包括 3 个部分：处理写请求，管理日志复制，不断发送心跳信息通知其他节点”我是领导者，我还活者，你们现在不要发起新的选举“。
- follower：跟随者，相当于普通群众，被动接收和处理来自领导者的消息。当领导者心跳超时时，就主动站出来，推荐自己当选候选人
- candidate：候选人，用于选举出一个新的 leader。候选人向其他节点发送投票（RequestVote RPC）消息，通知其他节点来投票，如果赢得子大多数选票，就升级为领导者。    

Raft 算法中，节点之间采用 RPC 进行通信，下面两种 RPC 是 Raft 基础功能的必要实现：   

- RequestVote RPC：请求投票 RPC，候选人在选举期间发起，用于通知其他节点投票
- AppendEntries RPC：日志复制 RPC，由领导者发起，用于复制日志和提供心跳消息。     


Raft 算法中还有个很重要的概念 -- term（任期），Raft 将时间分割为不同长度的任期，记作 term（任期），每个 term 由单调递增的数字（任期编号）标识。例如，节点 A 的任期编号为 1，任期编号随着选举的举行变化而变化，即每个 term 始于一个新的选举。    

看不懂看不懂了。。。。。     

## 第七章：容器技术概论


### 7.2 以容器构建系统

容器的本质是个特殊的进程，就是云时代的操作系统 Kubernetes 中的进程。      

容器的本质是对 cgroups 和 namespaces 所提供的隔离能力的一种封装，在 Docker 提倡的单进程封装的理念影响下，容器蕴含的隔离性也多了仅针对于单个进程的额外局限，然而 Linux 的 cgroups 和 namespaces 原本都是针对进程组而不仅仅是单个进程来设计的，同一个进程组中的多个进程天然就可以共享着相同的访问权限与资源配额。

如果现在我们把容器与进程在概念上对应起来，那容器编排的第一个扩展点，就是要找到容器领域中与“进程组”相对应的概念，这是实现容器从隔离到协作的第一步，在 Kubernetes 的设计里，这个对应物叫作 Pod。

Kubernetes 是典型的主从架构。有两部分组成：管理者被称为 Control Plane（控制平面）、被管理者称为 Node（节点）。    

![k8s](https://www.thebyte.com.cn/assets/k8s-K5U41NQB.png)     

**Control Plane**     

Control Plane 是集群管理者，在逻辑上只有一个(这不会导致单点故障？)。按照习惯称呼，我们也可把该计算机称之为 Master 节点。Control Plane 对节点进行统一管理，调度资源并操作 Pod，它的目标就是使得用户创建的各种 Kubernetes 对象按照其配置所描述的状态运行。它包含如下组件：        

- API Server： 操作 Kubernetes 各个资源的应用接口。并提供认证、授权、访问控制、API 注册和发现等机制。
- Scheduler（调度器）：负责调度 Pod 到合适的 Node 上。例如，通过 API Server 创建 Pod 后，Scheduler 将按照调度策略寻找一个合适的 Node。
- Controller Manager（集群控制器）：负责执行对集群的管理操作。例如，按照预期增加或者删除 Pod，按照既定顺序系统一系列 Pod。




**Node**    

Node 通常也被称为工作节点，可以有多个，用于运行 Pod 并根据 Control Plane 的命令管理各个 Pod，它包含如下组件：

- Kubelet 是 Kubernetes 在 Node 节点上运行的代理，负责所在 Node 上 Pod 创建、销毁等整个生命周期的管理。
- Kube-proxy 在 Kubernetes 中，将一组特定的 Pod 抽象为 Service，Kube-proxy 通过维护节点上的网络规则，为 Service 提供集群内服务发现和负载均衡功能。
Container runtime (容器运行时)：负责 Pod 和 内部容器的运行。在第七章已经介绍过各类容器运行时，Kubernetes 支持多种容器运行时，如 containerd、Docker 等。     


### 7.3 容器镜像

镜像主要是由镜像层和运行时配置两大部分组成。镜像层和运行时配置各自有一个唯一 Hash，这些 Hash 会被写进一个叫 Manifest 的 JSON 文件里，在 Pull 镜像时实际就是先拉取 Manifest 文件，然后再根据 Hash 去 Registry 拉取对应的镜像层/容器运行时配置。

### 7.5 容器间网络

Linux 网络虚拟化。     

网络命名空间（Network Namespace）是 Linux Kernel 提供的用于实现网络虚拟化的核心，它能创建多个隔离的网络空间，该网络空间内的防火墙、网卡、路由表、邻居表、协议栈与外部独立，不管是虚拟机还是容器，当运行在独立的命名空间时，就像是一台单独的物理主机。   


![network ns](https://www.thebyte.com.cn/assets/network-namespace-4hEnYVil.svg)    

由于不同的命名空间之间相互隔离，所以同一个宿主机之内的命名空间并不能直接通信，如果想与外界（例如其他网络命名空间、宿主机）进行通信，就需要在命名空间里面插入虚拟网卡（veth），然后再连接到虚拟交换机（Linux Bridge）。没错，这些操作完全和物理环境中的局域网配置一样，只不过全部是虚拟的，用软件实现的而已。     

严格来说，veth 一对设备，因而也常被称作 veth pair。简单理解 veth 就是一根带两个 Ethernet 网卡的网线，从一头发数据，另一头收数据，如果 veth-1 和 veth-2 是一对 veth 设备，veth-1 发送的数据会由 veth-2 收到，反之亦然。

既然有了虚拟网卡，很自然就想到让网卡接入交换机，以实现多个容器间的互相连接。在物理网络中，如果需要连接多个主机，我们会使用网桥（也可以理解为交换机）设备组成一个小型局域网。在 Linux 网络虚拟化系统中，也提供了网桥虚拟实现 Linux Bridge。

Linux Bridge 是 Linux Kernel 2.2 版本开始提供的二层转发工具，与物理交换机机制一致，能够接入任何二层的网络设备（无论是真实的物理设备，例如 eth0 或者虚拟设备，例如 veth、tap 等）。不过 Linux Bridge 与普通物理交换机还有有一点不同，普通的交换机只会单纯地做二层转发，Linux Bridge 却还能把发给它的数据包再发送到主机的三层协议栈中。    


部署 Docker 或者 Kubernetes 时，宿主机内的 cni0、docker0 就是它们创建的 Linux 网桥设备。

SDN 的核心思想是在物理网络之上，再构建一层虚拟化的网络，通过上层控制平面参与对网络的控制管理，以满足业务网络运维的需求。SDN 位于下层的物理网络被称为 Underlay，它着重解决网络的连通性，位于上层的逻辑网络被称为 overlay，它着重为应用提供与软件需求相符的传输服务和拓扑结构。由于跨主机的通信绝大多数都是 overlay 网络，所以在本节，笔者以 VXLAN 为例介绍 overlay 网络原理。   

![sdn](https://www.thebyte.com.cn/assets/overlay-IhkJ1Aho.svg)

VXLAN 你可能没有听说过，但 VLAN（Virtual Local Area Network，虚拟局域网）相信只要从事计算机行业的人都有所了解，VLAN 是一种早期的网络虚拟化技术，由于二层网络本身特性决定它非常依赖广播，但当设备非常多、广播又非常频繁的时候，很容易形成广播风暴，因此 VLAN 的首要职责是划分广播域，将同一个物理网络的设备区分出来。具体的做法是在以太网的报头增加 VLAN tag，让所有广播只针对相同的 VLAN tag 的设备生效，这样就缩小了广播域，也提高了安全性和可管理性。    

不过 VLAN 有一个非常明显的缺陷，就是 VLAN tag 的设计，当时的网络工程师未曾想到云计算在现在会如此普及，只有 12 位来存储 VLAN ID，标准定义中 VLAN 的数量只有 4000 个左右，这显然无法支持大型数据中心数以万记的设备数，另外，VLAN 的二层范围一般较小且固定，也无法支持虚拟机大范围的动态迁移。

为了解决上面这些问题，IETF 定义了 VXLAN 规范，这是三层虚拟化网络（Network Virtualization over Layer 3，NVO3）的标准技术规范之一，是一种典型的 overlay 网络。VXLAN 完美地弥补了 VLAN 的上述不足，一方面通过 VXLAN 中的 24 比特 VNI 字段（如图 1-5 所示）提供多达 16M 租户的标识能力，远大于 VLAN 的 4000；另一方面，VXLAN 本质上在两台交换机之间构建了一条穿越数据中心基础 IP 网络的虚拟隧道，将数据中心网络虚拟成一个巨型“二层交换机”，满足虚拟机大范围动态迁移的需求。      

虽然从名字上看，VXLAN 是 VLAN 的一种扩展协议，但 VXLAN 内在已经与 VLAN 迥然不同，VXLAN 本质上是一种隧道封装技术，它使用 TCP/IP 协议栈的惯用手法“封装/解封装技术”，将 L2 的以太网帧（Ethernet frames）封装成 L4 的 UDP 数据报，然后在 L3 的网络中传输，效果就像 L2 的以太网帧在一个广播域中传输一样，不再受数据中心传输的限制。VXLAN 的通信原理如下图所示，VXLAN 每个边缘入口都部署了一个 VTEP（VXLAN Tunnel Endpoints，VXLAN 隧道端点），VTEP 是 VXLAN 隧道的起点和终点，VXLAN 对用户原始数据帧的封装和解封装均在 VTEP 上进行，VTEP 既可以是一台独立的网络设备，也可以是在服务器中的虚拟交换机。源服务器发出的原始数据帧，在 VTEP 上被封装成 VXLAN 格式的报文，并在 IP 网络中传递到另外一个 VTEP 上，并经过解封转还原出原始的数据帧，最后转发给目的服务器。     

根据上面的网络配置模型，Pod 完成了所示的本地通信、同节点通信、跨节点通信。

- 本地通信就是 Pod 内部不同容器间的通信，同一个 Pod 内的容器共享同一个网络命名空间，所以它们之间通过 Loopback 回环接口，保证端口不冲突就能完成通信。
- 同节点之间的通信，Pod 通过 veth 设备全部关联在同一个cni0网桥中，实际上就是虚拟 Linux 网桥内部的通信，和现实中二层局域网内部设备之间通信没有差别。
- 跨节点的通信只能通过宿主机的物理网卡进行，cni0 网桥流转到宿主机的 eth0 接口，再发送给 VPC 路由，VPC 路由到目标节点。flannel 收到数据包之后解封，再发送到 cni0 网桥，然后完成通信。


### 7.7 编排调度模型

对于一个编排系统而言，资源管理至少要考虑这几个问题：资源模型的抽象（有何种资源、如何表示这些资源）；资源的调度（如何描述一个资源申请、如何描述一台 node 资源分配的状态以及调度的算法）。      

可压缩的资源典型的是 CPU，当此类资源超限时，进程使用 CPU 会被限制，应用表现变得卡顿，业务延迟明显增加，但进程不会被杀掉。在 Kubernetes 中，CPU 的计量单位为毫核（m），一个 Node 节点 CPU 核心数量乘以 1000，得到的就是该 Node 节点总的 CPU 总数量。你得注意，CPU 的计量是绝对值，绝对值的意思是无论容器运行在单核、双核机器上，500m CPU 表示的是相同的计算能力。

不可压缩的资源为内存、显存（GPU），当此类资源不足时，进程产生 OOM 问题（Out of Memory，内存溢出）并被杀掉。内存的计算单位为字节，计量方式有多种写法，譬如使用 M（Megabyte）、Mi（Mebibyte）以及不带单位的数字。        

注意 Mebibyte 和 Megabyte 的区分，123 Mi = 123*1024*1024B = 129 M，123 M 等于 1*1000*1000字节，所以 1M < 1Mi，显然使用带这个小 i 的更准确。

Kubernetes 抽象了两个概念 requests 和 limits 用以描述容器资源的分配。

- requests 容器需要的最小资源量。举例来讲，对于一个 Spring Boot 业务容器，这里的 requests 必须是容器镜像中 JVM 虚拟机需要占用的最少资源。
- limits 容器最大可以消耗的资源上限，防止过量消耗资源导致资源短缺甚至宕机。
