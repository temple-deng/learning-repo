# 深度学习

# 前言 深度学习与智能的本质

⼈⼯智能近期取得的进展得益于⼤脑逆向⼯程。分层神经⽹络模型的学习算法受到了神经元之间交流⽅式的启发，
并依据经验进⾏了改进。在⽹络内部，世界的复杂性转变为五彩缤纷的内部活动模式，这些模式是智能的元素。    

本书的第⼀部分提供了深度学习的动机和理解其起源所需的背景信息；第⼆部分解释了⼏种不同类型的神经
⽹络架构中的学习算法；第三部分则探讨了深度学习对我们当下⽣活产⽣的影响，以及未来若⼲年可能产⽣的
影响。     

# 第一部分 智能的新构想

要事年表

**1956年**    

达特茅斯⼈⼯智能夏季研究计划（The Dartmouth Artificial Intelligence Summer Research
Project）开启了⼈⼯智能领域的研究，并⿎舞了⼀代科学家探寻可以媲美⼈类智慧的信息技术的潜⼒。    

**1962年**    

弗兰克·罗森布拉特（Frank Rosenblatt）出版了《神经动⼒学原理：感知器和⼤脑机制的理论》
(Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms)，
该书介绍了⼀种应⽤于具有单层可变权重的神经⽹络模型的学习算法，该算法是今天的深度神经⽹络模型的
学习算法的前⾝。    

**1962年**    

⼤卫·休伯尔（David Hubel）和托斯坦·威泽尔（Torsten Wiesel）发表了《猫的视觉⽪质中的感受野、
双⽬ 互动和功能架构》（Receptive Fields,Binocular Interaction and Functional
Architecture in the Cat’s Visual Cortex）⼀⽂，第⼀次报道了由微电极记录的单个神经元的响应
特性。深度学习⽹络的架构类似于视觉⽪质的层次结构。   

**1969年**    

马⽂·明斯基（Marvin Minsky）和西摩尔·帕普特（Seymour Papert）出版了《感知器》 (Perceptrons)，
该书指出了单个⼈造神经元的计算极限，标志着神经⽹络领域寒冬的到来。    

**1979年**    

杰弗⾥·⾟顿和詹姆斯·安德森（James Anderson）在加州拉荷亚市（La Jolla）举办了“关联记忆的并⾏模型”
（Parallel Models of Associative Memory）研讨会，把新⼀代的神经⽹络先驱们聚集到了⼀起，同时也
推动⾟顿和安德森在1981年发表了同名系列研究著作。    

# 01 机器学习的崛起

如今的计算机不再是被动按照指令识别和驾驶，⽽是像⾃然界的⽣命由数百万年前开始进化那样，⾃主地从
经验中学习。是数据的井喷促成了这⼀技术进步。     

深度学习是机器学习的⼀个分⽀，它根植于数学、计算机科学和神经科学。深度学习的起源可以追溯到20世纪
50年代⼈⼯智能的诞⽣。关于如何构建⼈⼯智能，当时存在两种不同的观点：⼀种观点主张基于逻辑和计算机
程序，曾主宰⼈⼯智能的研究和应⽤数⼗年；另⼀种观点则主张直接从数据中学习，经历了更长时间的摸索
才逐渐成熟。    

⾃动驾驶汽车的不同之处就在于，当⼀辆汽车遇到罕见事件时，相应的学习体验会被传递给所有其他⾃动驾驶
汽车，这是⼀种集体智能。   

## 自然语言翻译：从语言到句子的飞跃

⼏年前，⾕歌的⼯程师意识到他们需要将这些计算密集型应⽤扩展到云端。他们开始着⼿设计⼀种⽤于深度
学习的专⽤芯⽚，并巧妙地设计了可以插⼊数据中⼼机架中的硬盘插槽的电路板。⾕歌的张量处理单元（TPU）
现在已配置在遍布全球的服务器上，让深度学习应⽤程序的性能得到了⼤幅改进。    

⾕歌最近推出了基于深度学习的最新版⾕歌翻译（Google Translate），代表了⾃然语⾔翻译质量的重⼤
飞跃。⼏乎⼀夜之间，语⾔翻译就从零散杂乱的拼凑短语，升级到了语意完整的句⼦。之前的计算机⽅法搜
索的是可以被⼀并翻译的词汇组合，但深度学习会在整个句⼦中寻找词汇之间的依赖关系。    

## 语音识别：实时跨文化交流不再遥远

计算机键盘会被⾃然语⾔接口取代。随着数字助⼿，如亚马逊的Alexa、苹果的Siri以及微软的Cortana
先后进⼊千家万户，这种取代已经在发⽣了。就如随着个⼈电脑的普及，打字机退出了历史舞台，有⼀天电脑
键盘也将成为博物馆的展品。    

## AI 医疗：医学诊断将更加准确

在不久的将来，任何⼀个拥有智能⼿机的⼈都可以拍下疑似⽪肤病变的照⽚，并⽴即进⾏诊断。   

把深度学习与⼈类专家的预测结合起来，准确度达到了0.995，⼏近完美。由于深度学习⽹络和⼈类专家查看
相同的数据的⽅式不同，⼆者相结合的效果⽐单独预测要好。这样⼀来，更多的⽣命得以被挽救。这表明在
未来，⼈类与机器将是合作⽽⾮竞争的关系。    

# 02 人工智能的重生

马⽂·明斯基是⼀位杰出的数学家，也是⿇省理⼯学院⼈⼯智能实验室（以下简称MIT AI Lab）的创始⼈之⼀。   

## 计算机视觉的进步

计算机视觉研究中最早的想法之⼀是将物体的模板与图像中的像素进⾏匹配，但是这种⽅法收效甚微，因为同
⼀物体不同⾓度的两个图像中的像素并不匹配。   

计算机视觉的进步是通过关注特征⽽⾮像素来实现的。⼀个好的特征是指⼀种鸟类独有的特征，但是如果在别
的种类中也可以找到这些特征，那么就要靠翼带、眼纹和翼斑的独特标记组合来区分。当这些标记组合为近亲种
类所共享时，就要根据叫声和歌声进⼀步区分。鸟类的草图或彩绘能更好地将我们的注意⼒引导到相关的区别
特征上，相⽐之下，鸟类照⽚⾥则布满了数百个不太相关的特征。   

这种基于特征的识别⽅法存在的问题，不仅在于针对世界上数万种不同物体开发特征检测器是⾮常耗费⼈⼒的，
⽽且即便使⽤最精确的特征检测器，被部分遮挡住的物体的图像也会产⽣歧义，这使得识别混乱场景中的物体
成了计算机所⾯临的⼀项艰巨任务。   

## 早期人工智能发展缓慢

20世纪七⼋⼗年代流⾏的⼈⼯智能专家系统，被开发⽤以通过遵循⼀套规则来解决医学诊断等问题。研究⼈员
在⼈⼯智能发展早期尝试了许多不同的⽅法，这些⽅法通常都很巧妙，但并不实⽤。他们不仅低估了现实世界
问题的复杂性，⽽且提出的解决⽅案⽆法进⾏⼤规模应⽤。在复杂的领域中，规则的数量可能会⾮常庞⼤，
并且随着新的事实的出现⽽不断被添加，跟踪所有规则的例外情况并与之互动变得⼗分不切实际。    

早期AI进展得如此缓慢的另⼀个原因是，数字计算机还处于⾮常原始的阶段，并且以今天的标准来看存储器
成本⼗分⾼昂。但是由于数字计算机在逻辑运算、符号操作和规则应⽤⽅⾯⾮常⾼效，这些计算原语在20世纪
会受到青睐并不令⼈惊讶。    

那些试图编写具有⼈类智能的计算机程序的AI先驱，本⾝并不太关⼼⼤脑是如何实现智能⾏为的。⼤脑功能的
基本原理在20世纪50年代才慢慢开始被揭开，相关领域的带头⼈包括艾伦·霍奇⾦（Alan Hodgkin）和安德鲁·赫胥黎
（Andrew Huxley），他们解释了⼤脑信号是如何通过神经中的电脉冲来实现远距离传送的，⽽另⼀带头⼈
伯纳德·卡茨（Bernard Katz）则发现了这些电⼦信号如何经突触转化为化学信号来实现神经元间的通信。    

## 从神经网络到人工智能

在视觉⽪层内部，神经元呈多层次排列结构。随着感官信息在⽪层间层层传递，对世界的呈现也变得越来越抽
象。⼏⼗年来，随着神经⽹络模型层数的增加，其性能也在不断提⾼，直到最终达到了⼀个临界点，让我们能
够解决在20世纪80年代只能幻想却⽆法解决的问题。深度学习可以⾃动找出能区分图像中不同物体的优质特
征的过程，这就是今天的计算机视觉⽐5年前好得多的原因。    

# 03 神经网络的黎明

## 从样本中学习

尽管我们对⼤脑功能缺乏⾜够的了解，但神经⽹络的AI先驱们依然依靠着神经元的绘图以及它们相互连接的
⽅式，进⾏着艰难的摸索。康奈尔⼤学的弗兰克·罗森布拉特是最早模仿⼈体⾃动图案识别视觉系统架构的⼈
之⼀。他发明了⼀种看似简单的⽹络感知器（perceptron），这种学习算法可以学习如何将图案进⾏分类，
例如识别字母表中的不同字母。算法是为了实现特定⽬标⽽按步骤执⾏的过程，就像烘焙蛋糕的⾷谱⼀样。   

如果你了解了感知器如何学习图案识别的基本原则，那么你在理解深度学习⼯作原理的路上已经成功了⼀半。
感知器的⽬标是确定输⼊的图案是否属于图像中的某⼀类别（⽐如猫）。⽅框3.1解释了感知器的输⼊如何通过
⼀组权重，来实现输⼊单元到输出单元的转换。权重是对每⼀次输⼊对输出单元做出的最终决定所产⽣影响的
度量，但是我们如何找到⼀组可以将输⼊进⾏正确分类的权重呢？   

@TODO:   

⼯程师解决这个问题的传统⽅法，是根据分析或特定程序来⼿动设定权重。这需要耗费⼤量⼈⼒，⽽且往往
依赖于直觉和⼯程⽅法。另⼀种⽅法则是使⽤⼀种从样本中学习的⾃动过程，和我们认识世界上的对象的⽅法
⼀样。需要很多样本来训练感知器，包括不属于该类别的反⾯样本，特别是和⽬标特征相似的，例如，如果识别
⽬标是猫，那么狗就是⼀个相似的反⾯样本。这些样本被逐个传递给感知器，如果出现分类错误，算法就会⾃
动对权重进⾏校正。话说，权重值有那么多，分类错误后，如何知道修正哪个权重呢。       

感知器是具有单⼀⼈造神经元的神经⽹络，它有⼀个输⼊层，和将输⼊单元和输出单元相连的⼀组连接。感知
器的⽬标是对提供给输⼊单元的图案进⾏分类。输出单元执⾏的基本操作是，把每个输⼊（xn）与其连接强度
或权重（wn）相乘，并将乘积的总和传递给输出单元。上图中，输⼊的加权和（∑i=1,…,n wi xi）与阈值θ
进⾏⽐较后的结果被传递给阶跃函数。如果总和超过阈值，则阶跃函数输出“1”，否则输出“0”。例如，输⼊
可以是图像中像素的强度，或者更常见的情况是，从原始图像中提取的特征，例如图像中对象的轮廓。每次输
⼊⼀个图像，感知器会判定该图像是否为某类别的成员，例如猫类。输出只能是两种状态之⼀，如果图像处于
类别中，则为“开”，否则为“关”。“开”和“关”分别对应⼆进制值中的1和0。感知器学习算法可以表达为:   

δwi=αδxi   
δ=output–teacher    

这⾥，output（输出值）和 teacher（实际值）都是⼆进制的，所以根据差值，如果输出正确，δ=0，如果
输出不正确，δ=+1或者–1。    

如果对感知器学习的这种解释还不够清楚，我们还可以通过另⼀种更简洁的⼏何⽅法，来理解感知器如何学习
对输⼊进⾏分类。对于只有两个输⼊单元的特殊情况，可以在⼆维图上⽤点来表⽰输⼊样本。每个输⼊都是图
中的⼀个点，⽽⽹络中的两个权重则确定了⼀条直线。感知器学习的⽬标是移动这条线，以便清楚地区分正负
样本（见图3–5）。对于有三个输⼊单元的情况，输⼊空间是三维的，感知器会指定⼀个平⾯来分隔正负训练
样本。在⼀般的情况下，即使输⼊空间的维度可能相当⾼且⽆法可视化，同样的原则依然成⽴。   

@TODO:   

最终，如果解决⽅案是可⾏的，权重将不再变化，这意味着感知器已经正确地将训练集中的所有样本进⾏了
分类。但是，在所谓的“过度拟合”（overfitting）中，也可能没有⾜够的样本，⽹络仅仅记住了特定的样本，
⽽不能将结论推⼴到新的样本。为了避免过度拟合，关键是要有另⼀套样本，称为“测试集”（test set），
它没有被⽤于训练⽹络。训练结束时，在测试集上的分类表现，就是对感知器是否能够推⼴到类别未知的新样
本的真实度量。泛化（generalization）是这⾥的关键概念。在现实⽣活中，我们⼏乎不会在同样的视⾓
看到同⼀个对象，或者反复遇到同样的场景，但如果我们能够将以前的经验泛化到新的视⾓或场景中，我们就
可以处理更多现实世界的问题。    

## 利用感知器区分性别




