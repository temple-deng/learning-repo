# 第十四章 磁盘配额

## 14.1 磁盘配额的应用与操作

### 14.1.1 说明是 Quota

在	Linux	系统中,由于是多用户多任务的环境,所以会有多人共同使用一个硬盘空间的情况
发生, 如果其中有少数几个使用者大量的占掉了硬盘空间的话,那势必压缩其他使用者的使
用权力!因此管理员应该适当的限制硬盘的容量给使用者,以妥善的分配系统资源。     

Quota 比较常用的几种情况是：    

+ 针对 web 服务器，例如每个人的网页空间的容量限制（什么鬼）
+ 针对邮件服务器，例如每个人的邮件空间限制
+ 针对文件服务器，例如每个人最大的可用网络硬盘空间    

上面讲的是针对网络服务的设计，如果是针对 Linux 主机上面的设置那么使用的场景有：    

+ 限制某一群组所能使用的最大磁盘配额	(使用群组限制)
+ 限制某一使用者的最大磁盘配额	(使用使用者限制)
+ 限制某一目录的最大磁盘配额   

Quota 可以通过限制 inode 数量来限制文件的数量，或者限制 block 的数量来限制容量。     

不过是 inode/block，限制值都有两个，分别是 soft 与 hard，通常 hard 限制值要比 soft 高。hard 表示用户的用量不能超过这个限制值。soft表示用户在超过soft且低于 hard 的限值之间时，每次使用者登录系统，系统会发出警告信息，且会给予一个宽限时间。    

略。待补充。    

## 14.2 软件磁盘阵列 (Software RAID)    

RAID 即 Redundant Arrays of Inexpensive Disks，RAID 可以通过软件或者硬件将多个较小的磁盘整合成一个较大的磁盘设备，常见的RAID level 如下：    

+ RAID-0（等量模式, stripe）：性能最佳    
这种模式使用相同型号与容量的磁盘来组成时，效果最佳。这种模式的RAID 会将磁盘先切出等量的区块，然后当
一个文件要写入 RAID 时，该文件会依据区块的大小切割好，之后再依序放到各个磁盘里面去。由于每个磁盘会交错的存放数据，因此当你的数据要写入 RAID 时，数据会被等量的放置到各个磁盘上面。举例来说，有两颗磁盘组成 RAID-0，当我们有100M 的数据要写入时，每个磁盘会被分配到 50M 的存储量。   

+ RAID-1（映射模式，mirror）：完整备份     
这种模式也是需要相同的磁盘容量的。这种模式主要是让同一份数据，完整的保存在两颗磁盘上面。举例来说，如果
我有100M 的文件，有两颗磁盘组成的 RAID-1，那么这两颗磁盘会同步写入100M 到磁盘中。      

+ RAID 1+0，RAID 0+1    
所谓 RAID 1+0, 就是：1.先让两颗磁盘组成 RAID1，并且这样的设置共有两种；2.将这两组 RAID1 再组成一组 RAID0。反过来说，RAID 0+1 就是先组成 RAID-0再组成 RAID-1。   

+ RAID 5:性能与数据备份的均衡考虑     
RAID-5 至少需要三颗以上的磁盘才能够组成这种类型的磁盘阵列。这种磁盘阵列的数据写入有点类似 RAID-0，不过在每个循环的写入过程中，在每颗磁盘还加入一个同位检查数据，这个数据会记录其他磁盘的备份数据。    

+ RAID 6 与 RAID 5 类似，但是需要两颗磁盘来保存校验信息，应该是一样的内容，这样的话应该最少得4颗磁盘      

### 14.2.3 软件磁盘阵列的设置

使用 `mdadm` 指令创建 RAID：     

```
#mdadm  --detail /dev/md0
#mdadm  --create /dev/md[0-9] --auto=yes --level=[015] --chunk=NK \
> --raid-devices=N --spare-ddevices=N /dev/sdx /dev/hdx

--create：  创建 RAID
--auto=yes：  决定创建后面接的软件磁盘阵列设备，即 /dev/md0, /dev/md1...
--chunk=NK：设备的 chunk 大小
--raid-devices=N：使用几个磁盘作为磁盘阵列的设备
--spare-devices=N：使用几个磁盘作为备用设备
--level=[015]：磁盘阵列的等级，话说怎么设置10或者01
--detail：后面所接的那个磁盘阵列设备的详细信息
```     

上面的语法中，最后面会接许多的设备文件名，这些设备文件名可以是整颗磁盘，例如 /dev/sdb，也可以是分区，例如 /dev/sdb1。不过这些设备文件名的总数必须要等于 --raid-devices 与 --spare-devices 的个数总和才行。    

```
#mdadm --create /dev/md0 --auto=yes --level=5 --chunk=256K \
--raid-devices=4 --spare-devices=1 /dev/vda{5,6,7,8,9}
```     

剩下的略了。    

## 14.3 逻辑卷轴管理员(Logical Volume Manager)    

LVM 的做法是将几个实体的分区或磁盘通过软件组合称为一块看起来是独立的大磁盘(VG)，然后将这块大磁盘再经过分区称为可使用分区(LV)，最终就能够挂载使用了。    

+ Physical Volume, PV 实体卷轴    
我们实际的分区或磁盘需要调整系统识别码成为 8e（LVM 的识别码），然后再经过 `pvcreate` 命令将其转成 LVM 最底层的实体卷轴，之后才能够将这些 PV 加以利用。    

+ Volume Group, VG 卷轴数组    
所谓的 LVM 大磁盘就死将许多 PV 整合成 VG。    

+ Physical Extend, PE 实体范围区块     
LVM 默认使用 4M 的 PE 区块，而 LVM 的 LV 在32 位系统最多能含有 65534 个 PE，PE 是整个LVM 最小的存储区块，也就说，其实我们的文件数据都是借由写入 PE 来处理的。     

+ Logical Volume, LV 逻辑卷轴     
最终的 VG 还会被切成 LV，这个LV就是最后可以被格式化使用的类似分区的东西。     

之前提到了 LVM 可弹性的变更文件系统的容量，即使通过交换 PE 来进行数据转换，将原本 LV 内的 PE 转移到其他设备中以降低 LV 容量，或将其他设备的 PE 加到此 LV 中以加大容量。    

当数据写入 LV 中时，有两种写入磁盘的方式：    

+ 线性模式：加入我们将 /dev/vda1，/dev/vdb1 这两个分区加入到 VG 当中，并且整个 VG 只有一个 LV 时，那么所谓的线性模式就是：当 /dev/vda1 的容量用完之后，/dev/vdb1 的磁盘才会被使用到。
+ 交错模式：将一笔数据分成来部分，分别写入 /dev/vda1 和 /dev/vdb1，类似 RAID0。     

剩下的略。     


